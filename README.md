### Hello, everyone ðŸ‘‹

<!--
**KimManjin/KimManjin** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->

I'm Manjin Kim, a Ph.D student in [Pohang University of Science and Technology (POSTECH)](https://www.postech.ac.kr/eng/), South Korea. I am a member of the [computer vision lab](http://cvlab.postech.ac.kr/lab/) at POSTECH, under supervision of Professor [Minsu Cho](http://cvlab.postech.ac.kr/~mcho/). My research interest is in learning video representation and its applications.

---------------------------------------------

## Research Interests
* Video representation learning
* Motion feature learning
* Multi-modal learning

---------------------------------------------

## Publications
- **Learning correlation structures for vision transformers**   
  **Manjin Kim**, Paul Hongsuck Seo, Cordelia Schmid, Minsu Cho, _under review_ 2024.
- **[Future transformer for long-term action anticipation](https://arxiv.org/abs/2205.14022)** &#91;[code](https://github.com/gongda0e/FUTR)&#93;  
  Dayoung Gong, Joonseok Lee, **Manjin Kim**, Seongjong Ha, Minsu Cho, _CVPR_ 2022.
- **[Relational self-attention: what's missing in attention for video understanding](https://arxiv.org/abs/2111.01673)** \[[code](https://github.com/KimManjin/RSA)\]  
  **Manjin Kim\***, Heeseung Kwon\*, Chunyu Wang, Suha Kwak, and Minsu Cho (* equal contribution), _NeurIPS_ 2021.     
- **[Learning self-similarity in space and time as as generalized motion for video action recognition](https://arxiv.org/abs/2102.07092)** \[[code](https://github.com/arunos728/SELFY)\]  
  Heeseung Kwon\*, **Manjin Kim\***, Suha Kwak, and Minsu Cho (* equal contribution), _ICCV_ 2021.
- **[MotionSqueeze: neural motion feature learning for video understanding](https://arxiv.org/abs/2007.09933)** \[[code](https://github.com/arunos728/MotionSqueeze)\]  
  Heeseung Kwon, **Manjin Kim**, Suha Kwak, and Minsu Cho, _ECCV_ 2020.

---------------------------------------------

## Industry experiences
- **Student Researcher**, [Google Research]([https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/](https://research.google/)), France (Jul. 2022 - Jan. 2023)
    + Developed a multimodal long-form video captioning system.
    + Host: [Paul Hongsuck Seo](https://phseo.github.io/)
- **Research Intern**, [Microsoft Research Asia (MSRA)](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/), remote (Dec. 2020 - June. 2021)
    + Developed a dynamic neural feature transform method, called Relational Self-Attention.
    + Mentor: [Chunyu Wang](https://www.microsoft.com/en-us/research/people/chnuwa/)
- **Research Intern**, [LG CNS](https://www.lgcns.com/EN/Home), Korea (Jun. 2018 - Aug. 2018)
    + Developed a video data augmentation system using CycleGAN.

---------------------------------------------

#### Contact
* CV: [CV](https://github.com/KimManjin/KimManjin/blob/main/ManjinKim_resume.pdf)
* e-mail: mandos@postech.ac.kr
* google scholar: [https://scholar.google.com/citations?user=kqddtlwAAAAJ&hl=en&oi=ao](https://scholar.google.com/citations?user=kqddtlwAAAAJ&hl=en&oi=ao)
